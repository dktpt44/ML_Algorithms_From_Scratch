{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d949f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba43f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# node for building the tree \n",
    "class TreeNode(): \n",
    "  # constructor\n",
    "  def __init__(self, featureIndex=None, conditionVal=None, leftNode=None, rightNode=None, value=None):      \n",
    "    self.featureIndex = featureIndex\n",
    "    self.conditionVal = conditionVal\n",
    "    self.leftNode = leftNode\n",
    "    self.rightNode = rightNode\n",
    "    self.value = value # only leaf node holds value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ab67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier():\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, nMinNode, X, Y):\n",
    "        self.nMinNode = nMinNode\n",
    "        self.root = self.buildDecisionTreeRecursive(X,Y)\n",
    "\n",
    "    # average value calculator\n",
    "    def getAvgValues(self, xCol):\n",
    "        n = len(xCol)\n",
    "        meanVals = np.zeros(n)\n",
    "        for i in range(n-1):\n",
    "            meanVals[i] = (xCol[i]+xCol[i+1])/2\n",
    "        return meanVals\n",
    "    \n",
    "    # calculate entropy\n",
    "    def entropy(self, y):\n",
    "        categories = np.unique(y)\n",
    "        entropy = 0\n",
    "        for catg in categories:\n",
    "            prob = len(y[y == catg]) / len(y)\n",
    "            entropy += -prob * np.log2(prob)\n",
    "        return entropy\n",
    "\n",
    "    # compute information gain\n",
    "    def infoGain(self, p, l, r):\n",
    "        return (self.entropy(p) - ((len(l) / len(p))*self.entropy(l) + (len(r) / len(p))*self.entropy(r)))\n",
    "\n",
    "    # split the node\n",
    "    def splitNode(self, X,y, m):\n",
    "        \n",
    "        # holds the information about current split\n",
    "        retCondVal = None\n",
    "        retFeatureIndx = None\n",
    "        maxGain = -np.inf\n",
    "        \n",
    "        # for each feature\n",
    "        for i in range(m):\n",
    "\n",
    "            uniqueVals = np.unique(X[:, i])\n",
    "            # possible condition values for making decisions\n",
    "            possibleCondVal = self.getAvgValues(uniqueVals)\n",
    "\n",
    "            # loop over all the feature values present in the data\n",
    "            for cond in possibleCondVal:\n",
    "                li = X[:, i] <= cond\n",
    "                ri = X[:,i] > cond\n",
    "\n",
    "                # check if it is required to split \n",
    "                if (len(li)!=0 and len(ri)!=0):\n",
    "                    infoGain = self.infoGain(y, y[li], y[ri])\n",
    "                    # update if gain is higher\n",
    "                    if infoGain>maxGain:\n",
    "                        retFeatureIndx = i\n",
    "                        retCondVal = cond\n",
    "                        maxGain = infoGain        \n",
    "        return retCondVal, retFeatureIndx\n",
    "    \n",
    "\n",
    "    # build the decision tree    \n",
    "    def buildDecisionTreeRecursive(self, X, y, depth=0):\n",
    "        n, m = np.shape(X)\n",
    "        # check if all y's are the same\n",
    "        if(np.all(y==y[0])):\n",
    "            return TreeNode(value=y[0])\n",
    "        # check if splitting condition is true \n",
    "        if n>=self.nMinNode:\n",
    "            retCondVal, retFeatureIndx = self.splitNode(X, y, m)\n",
    "            leftNodeValIndx = X[:, retFeatureIndx] < retCondVal\n",
    "            # build tree recrusively\n",
    "            leftTree = self.buildDecisionTreeRecursive(X[leftNodeValIndx],y[leftNodeValIndx], depth+1)\n",
    "            rightTree = self.buildDecisionTreeRecursive(X[~leftNodeValIndx],y[~leftNodeValIndx] , depth+1)\n",
    "            return TreeNode(retFeatureIndx, retCondVal, leftTree, rightTree)\n",
    "        \n",
    "        # else condition \n",
    "        z = list(y)\n",
    "        # put the value that has the most count\n",
    "        return TreeNode(value=max(z, key=z.count))\n",
    "        \n",
    "    # recursive tree traversal\n",
    "    def traverseTree(self, x, currNode):\n",
    "        # if leaf node \n",
    "        if currNode.value!=None: \n",
    "            return currNode.value\n",
    "        if x[currNode.featureIndex]<=currNode.conditionVal:\n",
    "            return self.traverseTree(x, currNode.leftNode)\n",
    "        else:\n",
    "            return self.traverseTree(x, currNode.rightNode)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self.traverseTree(x, self.root) for x in X]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "682681d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 1)\n",
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "\n",
      "Accuracy:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "def loadAndRunIris():\n",
    "    # load the dataset\n",
    "    df = pd.read_csv('iris.csv', header=None)\n",
    "    # raw x and y\n",
    "    rX = df.iloc[:, :-1].values\n",
    "    ry = df.iloc[:, -1].values.reshape(-1,1)\n",
    "    print(rX.shape)\n",
    "    print(ry.shape)\n",
    "    print(np.unique(ry))\n",
    "\n",
    "    # split into training and testing sets\n",
    "    X, xT, y, yT = train_test_split(rX, ry, test_size=0.2, random_state=50)\n",
    "    \n",
    "    # create and instance of decision tree\n",
    "    classifier = MyDecisionTreeClassifier(3,X,y)\n",
    "\n",
    "    # # make predictions on the testing data\n",
    "    yP = classifier.predict(xT) \n",
    "    print(\"\\nAccuracy: \",accuracy_score(yT, yP))\n",
    "\n",
    "    return\n",
    "\n",
    "loadAndRunIris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "920ad0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  nmin  Accuracy  Std Dev\n",
      "0    5  0.946667     0.04\n",
      "0   10  0.946667     0.04\n",
      "0   15  0.946667     0.04\n",
      "0   20  0.946667     0.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def doTenFoldValidation():\n",
    "      # load the dataset\n",
    "    df = pd.read_csv('iris.csv', header=None)\n",
    "\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values.reshape(-1,1)\n",
    "\n",
    "    nmin_range = [5, 10, 15, 20]\n",
    "\n",
    "    # create an empty dataframe to store the results\n",
    "    results_df = pd.DataFrame(columns=['nmin', 'Accuracy', 'Std Dev'])\n",
    "\n",
    "    for nmin in nmin_range:\n",
    "        # calculate the minimum number of instances\n",
    "        nMinval = int(len(X) * (nmin / 100))\n",
    "\n",
    "        totalAccuracy = []\n",
    "        # divide data into k folds\n",
    "        kFoldSet = KFold(n_splits=10, random_state=41,shuffle=True)\n",
    "        for i, (trainIndx, testIndx) in enumerate(kFoldSet.split(X)):\n",
    "            # divide data \n",
    "            xTrainSet, xTestSet = X[trainIndx],X[testIndx]\n",
    "            yTrainSet, yTestSet = y[trainIndx],y[testIndx]\n",
    "            classifier = MyDecisionTreeClassifier(nMinval,xTrainSet,yTrainSet)\n",
    "            # predect \n",
    "            yP = classifier.predict(xTestSet) \n",
    "            totalAccuracy.append(accuracy_score(yTestSet,yP))\n",
    "\n",
    "        accuracy = np.mean(totalAccuracy)\n",
    "        std_dev = np.std(totalAccuracy)\n",
    "\n",
    "        # add the results to the dataframe\n",
    "        results_df = pd.concat([results_df, pd.DataFrame.from_records([{'nmin': nmin, 'Accuracy': accuracy, 'Std Dev': std_dev}])])\n",
    "\n",
    "    # print the results dataframe\n",
    "    print()\n",
    "    print(results_df)\n",
    "\n",
    "    return\n",
    "\n",
    "doTenFoldValidation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d700b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 57)\n",
      "(4601, 1)\n",
      "[0 1]\n",
      "\n",
      "Accuracy:  0.9272529858849077\n"
     ]
    }
   ],
   "source": [
    "def loadAndRunSpamBase():\n",
    "    # load the dataset\n",
    "    df = pd.read_csv('spambase.csv', header=None)\n",
    "    # raw x and y\n",
    "    rX = df.iloc[:, :-1].values\n",
    "    ry = df.iloc[:, -1].values.reshape(-1,1)\n",
    "    print(rX.shape)\n",
    "    print(ry.shape)\n",
    "    print(np.unique(ry))\n",
    "\n",
    "    # split into training and testing sets\n",
    "    X, xT, y, yT = train_test_split(rX, ry, test_size=0.2, random_state=50)\n",
    "    \n",
    "    # create and instance of decision tree\n",
    "    classifier = MyDecisionTreeClassifier(3,X,y)\n",
    "\n",
    "    # # make predictions on the testing data\n",
    "    yP = classifier.predict(xT) \n",
    "    print(\"\\nAccuracy: \",accuracy_score(yT, yP))\n",
    "\n",
    "    return\n",
    "\n",
    "loadAndRunSpamBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0b47b8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmin 5 Accuracy 0.9019800999717063 Std Dev 0.019246776673454327\n",
      "nmin 10 Accuracy 0.8945892671885316 Std Dev 0.022147873345992265\n",
      "nmin 15 Accuracy 0.8780675280580967 Std Dev 0.02373490398152029\n",
      "nmin 20 Accuracy 0.8428614543053852 Std Dev 0.03411897686536198\n",
      "nmin 25 Accuracy 0.8239484108271243 Std Dev 0.01615110738524305\n",
      "\n",
      "  nmin  Accuracy   Std Dev\n",
      "0    5  0.901980  0.019247\n",
      "0   10  0.894589  0.022148\n",
      "0   15  0.878068  0.023735\n",
      "0   20  0.842861  0.034119\n",
      "0   25  0.823948  0.016151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def doTenFoldValidation2():\n",
    "      # load the dataset\n",
    "    df = pd.read_csv('spambase.csv', header=None)\n",
    "\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values.reshape(-1,1)\n",
    "\n",
    "    nmin_range = [5, 10, 15, 20, 25]\n",
    "\n",
    "    # create an empty dataframe to store the results\n",
    "    results_df = pd.DataFrame(columns=['nmin', 'Accuracy', 'Std Dev'])\n",
    "\n",
    "    for nmin in nmin_range:\n",
    "        # calculate the minimum number of instances\n",
    "        nMinval = int(len(X) * (nmin / 100))\n",
    "\n",
    "        totalAccuracy = []\n",
    "        # divide data into k folds\n",
    "        kFoldSet = KFold(n_splits=10, random_state=41,shuffle=True)\n",
    "        for i, (trainIndx, testIndx) in enumerate(kFoldSet.split(X)):\n",
    "            # divide data \n",
    "            xTrainSet, xTestSet = X[trainIndx],X[testIndx]\n",
    "            yTrainSet, yTestSet = y[trainIndx],y[testIndx]\n",
    "            classifier = MyDecisionTreeClassifier(nMinval,xTrainSet,yTrainSet)\n",
    "            # predect \n",
    "            yP = classifier.predict(xTestSet) \n",
    "            totalAccuracy.append(accuracy_score(yTestSet,yP))\n",
    "\n",
    "        accuracy = np.mean(totalAccuracy)\n",
    "        std_dev = np.std(totalAccuracy)\n",
    "        print('nmin', nmin, 'Accuracy',accuracy, 'Std Dev',std_dev)\n",
    "\n",
    "        # add the results to the dataframe\n",
    "        results_df = pd.concat([results_df, pd.DataFrame.from_records([{'nmin': nmin, 'Accuracy': accuracy, 'Std Dev': std_dev}])])\n",
    "\n",
    "    # print the results dataframe\n",
    "    print()\n",
    "    print(results_df)\n",
    "\n",
    "    return\n",
    "\n",
    "doTenFoldValidation2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3024ce1ba369ad3c39b7fd70aa3226a2020022edfd0835bff81cbc893341e442"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
